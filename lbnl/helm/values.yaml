---
# Original in-house Postgres configuration.
postgres:
  deployment:
    replicaCount: 1
    image:
      repository: docker.io
      name: postgres
      tag: "11.5-alpine"
      pullPolicy: IfNotPresent
    containerPort: 5432

  service:
    type: ClusterIP
    port: 5432

  pvc_name: postgres-data
  dbmount: /var/lib/postgresql/data
  dbname: ochami

# Third-party Postgres subcharts
# Per https://github.com/bitnami/charts/issues/2876 Bitnami likely won't support
# multiple databases and users in the chart directly. You can use a single Postgres
# instance for multiple clients and databases if you specify a custom initdb string,
# which is essentially "roll your own SQL for initialization", thus losing any
# integration with the values.yaml-exposed user and database setup. You can create
# multiple subchart instances of the same subchart using the "alias" key in the
# dependency definition to use separate single-DB instances for each. Maybe this is
# desirable for isolation anyway? It's simpler to set up, if nothing else.
postgresql-hydra:
  enabled: true
  auth:
    # matches a static value in the deployment template
    username: hydra-user
    # this is already a static value
    database: hydradb
  image:
    tag: 16.4.0
  service:
    ports:
      postgresql: "5432"

postgresql-bss:
  enabled: true
  auth:
    # matches a static value in the deployment template
    username: bss-user
    # currently this is copy-pasta from another value, which is not great.
    # don't think there's any (good) way to share it between the two,
    # but using a static value instead is probably fine.
    database: bssdb
  image:
    tag: 16.4.0
  service:
    ports:
      postgresql: "5432"

postgresql-smd:
  enabled: true
  auth:
    # matches a static value in the deployment template
    username: smd-user
    # currently this is copy-pasta from another value, which is not great.
    # don't think there's any (good) way to share it between the two,
    # but using a static value instead is probably fine.
    database: hmsds
  image:
    tag: 16.4.0
  service:
    ports:
      postgresql: "5432"

smd:
  deployment:
    replicaCount: 1
    image:
      repository: ghcr.io/openchami
      name: smd
      tag: "v2.14.2"
      pullPolicy: IfNotPresent
    containerPort: 27779
  dbname: hmsds

  service:
    type: ClusterIP
    port: 27779
    annotations:

bss:
  deployment:
    replicaCount: 1
    image:
      repository: ghcr.io/openchami
      name: bss
      pullPolicy: IfNotPresent
      tag: "v1.29.1"
    containerPort: 27778
  dbname: bssdb

  service:
    type: ClusterIP
    port: 27778
    annotations:

krakend:
  deployment:
    replicaCount: 1
    image:
      repository: docker.io/devopsfaith
      name: krakend
      pullPolicy: IfNotPresent
      tag: "2.5.1"
    containerPort: 8080

  service:
    type: ClusterIP
    port: 80
    annotations:
      # Specify which BackendConfig to use for GKE to ensure the ingress
      # backend is healthy. The GKE docs say that sufficient annotation of the
      # KrakenD Deployment using the standard K8s API should be sufficient to
      # inspect the ingress backend, but I have not been successful in making
      # that work. Specifying the BackendConfig by hand seems to help.
      cloud.google.com/backend-config: '{"default": "krakend-hc-config"}'

# Parameters specific to deploying into Google Kubernetes Engine
gke:
  gateway:
    class: gke-l7-global-external-managed
    port: 80
  securityPolicy: lbnl-nersc

tftpd:
  deployment:
    replicaCount: 1
    image:
      repository: ghcr.io/openchami
      name: tftpd
      pullPolicy: IfNotPresent
      tag: "0.1"
    containerPort: 69

  service:
    type: LoadBalancer
    port: 69
    # Since GKE external network LoadBalancers are not proxied, it might be a
    # good idea to add an annotation like loadBalancerSourceRanges to limit who
    # can access the Service.
    annotations:

dnsmasq:
  deployment:
    replicaCount: 1
    image:
      repository: ghcr.io/openchami
      name: dnsmasq
      pullPolicy: IfNotPresent
      tag: dynamic
    dhcp_port: 67
    tftp_port: 69

  service:
    dhcp:
      type: LoadBalancer
      port: 67
      # Since GKE external network LoadBalancers are not proxied, it might be a
      # good idea to add an annotation like loadBalancerSourceRanges to limit who
      # can access the Service.
      annotations:

    tftp:
      type: LoadBalancer
      port: 69
      # Since GKE external network LoadBalancers are not proxied, it might be a
      # good idea to add an annotation like loadBalancerSourceRanges to limit who
      # can access the Service.
      annotations:

hydra:
  deployment:
    replicaCount: 1
    image:
      repository: docker.io/oryd
      name: hydra
      pullPolicy: IfNotPresent
      tag: "v2.2.0"
    containerPorts:
      public: 4444
      admin: 4445
      tokenuser: 5555

  service:
    type: ClusterIP
    ports:
      public: 4444
      admin: 4445
      tokenuser: 5555
    annotations:

hydra_consent:
  deployment:
    image:
      repository: docker.io/oryd
      name: hydra-login-consent-node
      pullPolicy: IfNotPresent
      tag: "v2.2.0"
    containerPorts:
      consent: 3000

  service:
    type: ClusterIP
    ports:
      consent: 3000
    annotations:

swiss_army_knife:
  deployment:
    replicaCount: 1
    image:
      repository: docker.io/leodotcloud
      name: swiss-army-knife
      pullPolicy: IfNotPresent
      tag: latest

lighttpd:
  deployment:
    replicaCount: 1
    image:
      repository: ghcr.io/openchami
      name: lighttpd
      pullPolicy: IfNotPresent
      tag: 0.1
    containerPort: 8080

  pvc_name: lighttpd
  mount_path: /var/www/html

  service:
    type: ClusterIP
    port: 80
    annotations:
      cloud.google.com/backend-config: '{"default": "lighttpd-backendconfig"}'
